print("Filtering on word matches")
dt <- dt[grepl(paste(tolower(words), collapse = "|"), tolower(tweet))]
if (nrow(dt) < 1) { return(NULL) }
}
if (!is.null(poly)) {
print("Filtering by polygon(s)")
if (ncol(poly) > 2) {warning("FYI: poly has more than 2 columns, all columns will be attached to dataset. Could cause memory issues.")}
dt_shp <- st_as_sf(dt, coords = c("lng", "lat"),  crs = 4326, agr = "constant", remove=F)
dt_shp <- st_transform(dt_shp, st_crs(poly))
dt_shp <- st_join(dt_shp, poly, left = F)
setDT(dt_shp)
dt <- dt_shp
# Can compute local date/time if we have geometry
dt[, c("localDate", "localTime") := get_local_time(dateTimeUTC, geometry)]
dt[, `:=`(geometry = NULL)]
if (nrow(dt) < 1) { return(NULL) }
}
dt
}
tz.rast <- raster(file.path(GEO, "tz_us/tz_us_MODIS1k_rasterized.tif"))
tz.vx <- velox(tz.rast)
tz_categories <- tz.rast@data@attributes[[1]]$category
get_local_time <- function (datetimeUTC, geom) {
rast.idx <- tz.vx$extract_points(geom)
tz_category <- tz_categories[rast.idx]
dt_tz <- data.table(datetimeUTC, tz_category)
# If no timezone, flag as a dummy, will blank after computation
dt_tz[is.na(tz_category), `:=`(badflag=TRUE, tz_category="America/Chicago")]
dt_tz[, `:=`(dateTimeLocal = as.POSIXct(datetimeUTC, tz = tz_category)),
by = list(tz_category = as.character(tz_category))]
dt_tz[, `:=`(localDate = as.IDate(datetimeUTC, tz = tz_category),
localTime = as.ITime(datetimeUTC, tz = tz_category)),
by = list(tz_category = as.character(tz_category))]
dt_tz[badflag == TRUE, `:=`(localDate=NA, localTime=NA)] # Blank times we flagged earlier
return(dt_tz[, list(localDate, localTime)])
}
pts <- st_sf(a=3, st_sfc(st_point(c(-105, 40)), st_point(c(-85, 40))), crs = 4326)
get_local_time(as.POSIXct("2001-01-01 10:01:01", tz = "UTC"), st_transform(pts, proj4string(tz.rast)))
# Pull tweets for the USA for a limited time frame
cbsas <- st_read("data/tl_2010_us_cbsa10/tl_2010_us_cbsa10.shp")[, "GEOID10"]
# metoo_tweets <- pull_tweets_usa(dates = as.IDate("2018-01-02"),
#                                 words = "#metoo",
#                                 poly = cbsas,
#                                 n_max = Inf)
metoo_tweets <- pull_tweets_usa(words = "#metoo",
poly = cbsas,
n_max = Inf)
saveRDS(metoo_tweets, "data/metoo_tweets.Rds")
pull_tweets_file <- function(in_file, col_types, col_names = NULL, words = NULL, poly = NULL, n_max = Inf, dates = NULL) {
# START DEBUG
# in_file <- files_old$fn[2]; col_types = cols_only(tweet_id = "c", user_id = "c", user_name = "c", tweet_datetime = "T", tweet = "c", lng="d", lat="d")
# in_file <- files_new$fn[1]; col_types = cols_only(id = "c", userid = "c", userName = "c", postedTime = "c", tweet = "c", lng="d", lat="d")
# col_names = c("tweet_id", "user_id", "user_name", "dateTimeUTC", "tweet", "lng", "lat")
# n_max = 100; words = c("and", "I"); poly = st_read("data/tl_2010_us_cbsa10/tl_2010_us_cbsa10.shp")[, "GEOID10"]
# END DEBUG
print(in_file)
dt <- read_csv(in_file, col_types = col_types, n_max = n_max)
if (nrow(dt) < 1) { return(NULL) }
setDT(dt)
setcolorder(dt, names(col_types$cols)) # Reorder, read_csv doesn't respect order of col_types
if (!is.null(col_names)) { setnames(dt, col_names) } # Rename if names passed
if (!all(c("dateTimeUTC", "lng", "lat", "tweet") %in% names(dt))) {
stop("One of the following required columns not found: dateTimeUTC, lng, lat, tweet")
}
if ("character" %in% class(dt$dateTimeUTC)) { # Fix datetime if it's a string
# Assumes format per https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object
print("Converting character datetime to POSIXct")
dt[, dateTimeUTC := as.POSIXct(dateTimeUTC, format="%a %b %d %H:%M:%S +0000 %Y", tz = "UTC")]
}
if (!is.null(dates)) {
# Because some of the files are compiled for a year-month, sometimes we want only a few days
# within that yearmonth. Unfortunately there's no way around loading the whole thing, but
# this lets us filter out irrelevant dates before the costly word and poly filtering
print("Filtering by date")
dt <- dt[as.IDate(dateTimeUTC) %in% dates]
if (nrow(dt) < 1) { return(NULL) }
}
# Filtering by polygon is time-consuming, so it's better to first filter on words
if (!is.null(words)) {
print("Filtering on word matches")
dt <- dt[grepl(paste(tolower(words), collapse = "|"), tolower(tweet))]
if (nrow(dt) < 1) { return(NULL) }
}
if (!is.null(poly)) {
print("Filtering by polygon(s)")
if (ncol(poly) > 2) {warning("FYI: poly has more than 2 columns, all columns will be attached to dataset. Could cause memory issues.")}
dt <- st_as_sf(dt, coords = c("lng", "lat"),  crs = 4326, agr = "constant", remove=F)
dt <- dt %>% st_transform(st_crs(poly)) %>% st_join(poly, left = F)
setDT(dt)
if (nrow(dt) < 1) { return(NULL) }
# Can compute local date/time if we have geometry
dt[, c("localDate", "localTime") := get_local_time(dateTimeUTC, geometry)]
dt[, `:=`(geometry = NULL)]
}
dt
}
# Pull tweets for the USA for a limited time frame
cbsas <- st_read("data/tl_2010_us_cbsa10/tl_2010_us_cbsa10.shp")[, "GEOID10"]
metoo_tweets <- pull_tweets_usa(dates = as.IDate(c("2015-01-02", "2015-01-08")),
words = "#metoo",
poly = cbsas,
n_max = Inf)
head(metoo)
head(metoo_tweets)
metoo_tweets <- pull_tweets_usa(words = c("#metoo", "#timesup"),
poly = cbsas,
n_max = Inf)
metoo_tweets <- pull_tweets_usa(words = c("#metoo", "#timesup"),
poly = cbsas,
n_max = 100)
# Pull tweets from various datasets
library(data.table)
library(dplyr)
library(readr)
library(sf)
library(raster)
library(velox)
library(ggplot2)
library(ggthemes)
library(stringr)
library(zoo)
TOLD <- file.path("/data2/tweets_bak/usa_geo_backup/data/db_to_csv")
TNEW <- file.path("/data2/tweets_bak/usa_geo_stream/flat/uncompressed")
GEO <- "~/Dropbox/01_Work/01_Current/99_Common/geo"
pull_tweets_usa <- function(dates = NULL, poly = NULL, words = NULL, n_max = Inf) {
# DEBUG
# dates <- as.IDate(c("2016-01-01", "2016-01-03", "2015-10-05", "2017-08-03", "2017-03-04"))
# words = c("and", "I")
# poly = st_read("data/tl_2010_us_cbsa10/tl_2010_us_cbsa10.shp")[, "GEOID10"]
# n_max = 100
# END DEBUG
if (is.null(dates)) { # Set dates from before Twitter started to present day
dates_use <- seq(as.IDate("2006-01-01"), as.IDate(Sys.Date()), 1)
} else {
dates_use <- dates
}
# Load the list of all files, keep only the ones with the right dates
# files_old are stored by date
files_old <- data.table(fn = list.files(TOLD, pattern = "\\.csv", full.names=T))
files_old[, date := as.IDate(str_extract(basename(fn), "[0-9]{4}-[0-9]{2}-[0-9]{2}"))]
files_old <- files_old[date %in% dates_use]
tweets_old <- rbindlist(lapply(files_old$fn, pull_tweets_file,
col_types = cols_only(tweet_id = "c", user_id = "c", user_name = "c", tweet_datetime = "T", tweet = "c", lng="d", lat="d"),
col_names = c("tweet_id", "user_id", "user_name", "dateTimeUTC", "tweet", "lng", "lat"),
words = words, poly = poly, n_max = n_max))
# New files are stored by ym (in retrospect, a stupid decision that I should fix later)
files_new <- data.table(fn = list.files(TNEW, pattern = "\\.csv", full.names=T))
temp <- str_extract(basename(files_new$fn), "[0-9]{4}[0-9]{2}")
files_new[, ym := as.yearmon(as.numeric(substr(temp, 1, 4)) + (as.numeric(substr(temp, 5, 6)) - 1) / 12)]
yms <- as.yearmon(dates_use)
files_new <- files_new[ym %in% yms]
# I pass dates rather than dates_use so that pull_tweets_file can skip the date filtering step if NULL is passed
tweets_new <- rbindlist(lapply(files_new$fn, pull_tweets_file,
col_types = cols_only(id = "c", userid = "c", userName = "c", postedTime = "c", tweet = "c", lng="d", lat="d"),
col_names = c("tweet_id", "user_id", "user_name", "dateTimeUTC", "tweet", "lng", "lat"),
words = words, poly = poly, n_max = n_max, dates = dates))
rbindlist(list(tweets_old, tweets_new))
}
pull_tweets_file <- function(in_file, col_types, col_names = NULL, words = NULL, poly = NULL, n_max = Inf,
dates = NULL) {
# START DEBUG
# in_file <- files_old$fn[2]; col_types = cols_only(tweet_id = "c", user_id = "c", user_name = "c", tweet_datetime = "T", tweet = "c", lng="d", lat="d")
# in_file <- files_new$fn[1]; col_types = cols_only(id = "c", userid = "c", userName = "c", postedTime = "c", tweet = "c", lng="d", lat="d")
# col_names = c("tweet_id", "user_id", "user_name", "dateTimeUTC", "tweet", "lng", "lat")
# n_max = 100; words = c("and", "I"); poly = st_read("data/tl_2010_us_cbsa10/tl_2010_us_cbsa10.shp")[, "GEOID10"]
# END DEBUG
print(in_file)
dt <- read_csv(in_file, col_types = col_types, n_max = n_max)
if (nrow(dt) < 1) { return(NULL) }
setDT(dt)
setcolorder(dt, names(col_types$cols)) # Reorder, read_csv doesn't respect order of col_types
if (!is.null(col_names)) { setnames(dt, col_names) } # Rename if names passed
if (!all(c("dateTimeUTC", "lng", "lat", "tweet") %in% names(dt))) {
stop("One of the following required columns not found: dateTimeUTC, lng, lat, tweet")
}
if ("character" %in% class(dt$dateTimeUTC)) { # Fix datetime if it's a string
# Assumes format per https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object
print("Converting character datetime to POSIXct")
dt[, dateTimeUTC := as.POSIXct(dateTimeUTC, format="%a %b %d %H:%M:%S +0000 %Y", tz = "UTC")]
}
if (!is.null(dates)) {
# Because some of the files are compiled for a year-month, sometimes we want only a few days
# within that yearmonth. Unfortunately there's no way around loading the whole thing, but
# this lets us filter out irrelevant dates before the costly word and poly filtering
print("Filtering by date")
dt <- dt[as.IDate(dateTimeUTC) %in% dates]
if (nrow(dt) < 1) { return(NULL) }
}
# Filtering by polygon is time-consuming, so it's better to first filter on words
if (!is.null(words)) {
print("Filtering on word matches")
dt <- dt[grepl(paste(tolower(words), collapse = "|"), tolower(tweet))]
if (nrow(dt) < 1) { return(NULL) }
}
if (!is.null(poly)) {
print("Filtering by polygon(s)")
if (ncol(poly) > 2) {warning("FYI: poly has more than 2 columns, all columns will be attached to dataset. Could cause memory issues.")}
dt <- st_as_sf(dt, coords = c("lng", "lat"),  crs = 4326, agr = "constant", remove=F)
dt <- dt %>% st_transform(st_crs(poly)) %>% st_join(poly, left = F)
setDT(dt)
if (nrow(dt) < 1) { return(NULL) }
# Can compute local date/time if we have geometry
dt[, c("localDate", "localTime") := get_local_time(dateTimeUTC, geometry)]
dt[, `:=`(geometry = NULL)]
}
dt
}
tz.rast <- raster(file.path(GEO, "tz_us/tz_us_MODIS1k_rasterized.tif"))
tz.vx <- velox(tz.rast)
tz_categories <- tz.rast@data@attributes[[1]]$category
get_local_time <- function (datetimeUTC, geom) {
rast.idx <- tz.vx$extract_points(geom)
tz_category <- tz_categories[rast.idx]
dt_tz <- data.table(datetimeUTC, tz_category)
# If no timezone, flag as a dummy, will blank after computation
dt_tz[is.na(tz_category), `:=`(badflag=TRUE, tz_category="America/Chicago")]
dt_tz[, `:=`(dateTimeLocal = as.POSIXct(datetimeUTC, tz = tz_category)),
by = list(tz_category = as.character(tz_category))]
dt_tz[, `:=`(localDate = as.IDate(datetimeUTC, tz = tz_category),
localTime = as.ITime(datetimeUTC, tz = tz_category)),
by = list(tz_category = as.character(tz_category))]
dt_tz[badflag == TRUE, `:=`(localDate=NA, localTime=NA)] # Blank times we flagged earlier
return(dt_tz[, list(localDate, localTime)])
}
pts <- st_sf(a=3, st_sfc(st_point(c(-105, 40)), st_point(c(-85, 40))), crs = 4326)
get_local_time(as.POSIXct("2001-01-01 10:01:01", tz = "UTC"), st_transform(pts, proj4string(tz.rast)))
# Pull tweets for the USA for a limited time frame
cbsas <- st_read("data/tl_2010_us_cbsa10/tl_2010_us_cbsa10.shp")[, "GEOID10"]
# metoo_tweets <- pull_tweets_usa(dates = as.IDate(c("2015-01-01", "2018-01-01")),
#                                 words = "#metoo",
#                                 poly = cbsas,
#                                 n_max = 10000000)
metoo_tweets <- pull_tweets_usa(words = c("#metoo", "#timesup"),
poly = cbsas,
n_max = 100)
metoo_tweets <- pull_tweets_usa(words = c("#metoo", "#timesup"),
poly = cbsas)
metoo_tweets_after2017 <- pull_tweets_usa(dates = seq(as.IDate("2017-01-01"), as.IDate(Sys.Date()), 1),
words = c("#metoo", "#timesup"),
poly = cbsas,
n_max = 100000)
ggplot(metoo_tweets %>% group_by(localDate) %>% summarise(N = n())) +
geom_line(mapping = aes(x = localDate, y= N)) +
theme_tufte()
ggplot(metoo_tweets_after2017 %>% group_by(localDate) %>% summarise(N = n())) +
geom_line(mapping = aes(x = localDate, y= N)) +
theme_tufte()
metoo_tweets_after2017 <- pull_tweets_usa(dates = seq(as.IDate("2017-01-01"), as.IDate(Sys.Date()), 1),
words = c("#metoo", "#timesup"),
poly = cbsas,
n_max = 1000000)
ggplot(metoo_tweets_after2017 %>% group_by(localDate) %>% summarise(N = n())) +
geom_line(mapping = aes(x = localDate, y= N)) +
theme_tufte()
metoo_tweets_after2017 <- pull_tweets_usa(dates = seq(as.IDate("2017-01-01"), as.IDate(Sys.Date()), 1),
words = c("#metoo", "#timesup"),
poly = cbsas)
saveRDS(metoo_tweets_after2017, "data/metoo_tweets_after2017.Rds")
# Plot
# metoo_tweets_before2017 <- readRDS("data/metoo_tweets_before2017.Rds")
# metoo_tweets_after2017 <- readRDS("data/metoo_tweets_after2017.Rds")
ggplot(metoo_tweets_after2017 %>% group_by(localDate) %>% summarise(N = n())) +
geom_line(mapping = aes(x = localDate, y= N)) +
theme_tufte()
install.packages("roxygen2")
# Pull tweets from various datasets
library(data.table)
library(dplyr)
library(readr)
library(sf)
library(raster)
library(velox)
library(ggplot2)
library(ggthemes)
library(stringr)
library(zoo)
library(scales)
library(tweetdata)
# Pull tweets for the USA for a limited time frame
cbsas <- st_read("data/tl_2010_us_cbsa10/tl_2010_us_cbsa10.shp")[, "GEOID10"]
metoo_tweets <- pull_tweets_usa(dates = as.IDate(c("2015-01-01", "2018-01-01")),
words = "#metoo",
poly = cbsas,
n_max = 100000)
metoo_tweets_before2017 <- readRDS("data/metoo_tweets_before2017.Rds")
metoo_tweets_after2017 <- readRDS("data/metoo_tweets_after2017.Rds")
metoo_tweets <- rbind(metoo_tweets_before2017, metoo_tweets_after2017)
metoo_tweets <- metoo_tweets %>% mutate(metoo = as.numeric(grepl("#metoo", tolower(tweet)))) %>%
mutate(metoo = factor(metoo, levels = c(0, 1), labels = c("#timesup", "#metoo")))
metoo_bydate <- metoo_tweets %>% group_by(localDate, metoo) %>% summarise(N = n())
metoo_bydate <- merge(metoo_bydate,
expand.grid(localDate = seq(min(metoo_tweets$localDate, na.rm = T),
max(metoo_tweets$localDate, na.rm = T), 1),
metoo = c("#metoo", "#timesup")),
by = c("localDate", "metoo"), all.y = T)
ggplot(metoo_bydate) + geom_line(mapping = aes(x = localDate, y = N, colour = metoo, group = metoo)) +
scale_y_continuous(name = "Daily count of geotagged #metoo and #timesup tweets", label = comma) +
labs(x = NULL) +
scale_colour_discrete(name = NULL) +
theme_tufte()
ggsave("results/daily-count-full.png", width = 6, height = 4)
ggplot(metoo_bydate %>% filter(localDate >= as.IDate("2017-10-01"))) + geom_line(mapping = aes(x = localDate, y = N, colour = metoo, group = metoo)) +
scale_y_continuous(name = "Daily count of geotagged #metoo and #timesup tweets", label = comma) +
scale_x_date(name = NULL, date_breaks = "1 month", date_labels =  "%b %Y") +
scale_colour_discrete(name = NULL) +
theme_tufte()
ggsave("results/daily-count-after-oct2017.png", width = 6, height = 4)
ggplot(metoo_bydate) + geom_line(mapping = aes(x = localDate, y = N, colour = metoo, group = metoo)) +
scale_y_continuous(name = "Daily count of geotagged tweets", label = comma) +
labs(x = NULL) +
scale_colour_discrete(name = NULL) +
theme_tufte()
ggsave("results/daily-count-full.png", width = 6, height = 4)
ggplot(metoo_bydate %>% filter(localDate >= as.IDate("2017-10-01"))) + geom_line(mapping = aes(x = localDate, y = N, colour = metoo, group = metoo)) +
scale_y_continuous(name = "Daily count of geotagged tweets", label = comma) +
scale_x_date(name = NULL, date_breaks = "1 month", date_labels =  "%b %Y") +
scale_colour_discrete(name = NULL) +
theme_tufte()
ggsave("results/daily-count-after-oct2017.png", width = 6, height = 4)
install.packages("rvest")
# Setup. Only needs to be run ONCE
source("config.R",echo=TRUE)
if (file.exists(datadir)){
} else {
dir.create(file.path(datadir))
}
if (file.exists(resultsdir)){
} else {
dir.create(file.path(resultsdir))
}
source("config.R",echo=TRUE)
# Create timezone raster
tz_rast <- raster("tz_us/tz_us_MODIS1k_rasterized.tif")
tz_vx <- velox(tz_rast)
# Create timezone raster
tz_rast <- raster("data/tz_us/tz_us_MODIS1k_rasterized.tif")
tz_vx <- velox(tz_rast)
tz_categories <- tz_rast@data@attributes[[1]]$category
#'
#' @import velox
#' @import sf
#'
#' @return data.table with localDate, localTime values
#' @export
#'
#' @examples
#' pts <- st_sf(a=3, st_sfc(st_point(c(-105, 40)), st_point(c(-85, 40))), crs = 4326)
#' get_local_time(as.POSIXct("2001-01-01 10:01:01", tz = "UTC"), pts)
get_local_time <- function (datetimeUTC, geom) {
geom <- st_transform(geom, tz_vx$crs)
rast.idx <- tz_vx$extract_points(geom)
tz_category <- tz_categories[rast.idx]
dt_tz <- data.table(datetimeUTC, tz_category)
# If no timezone, flag as a dummy, will blank after computation
dt_tz[is.na(tz_category), `:=`(badflag=TRUE, tz_category="America/Chicago")]
dt_tz[, `:=`(dateTimeLocal = as.POSIXct(datetimeUTC, tz = tz_category)),
by = list(tz_category = as.character(tz_category))]
dt_tz[, `:=`(localDate = as.IDate(datetimeUTC, tz = tz_category),
localTime = as.ITime(datetimeUTC, tz = tz_category)),
by = list(tz_category = as.character(tz_category))]
dt_tz[badflag == TRUE, `:=`(localDate=NA, localTime=NA)] # Blank times we flagged earlier
return(dt_tz[, .(localDate, localTime)])
}
#' @import data.table
#' @importFrom readr read_csv cols_only
#' @importFrom stringr str_extract
#'
#' @return data.table of tweets with "tweet_id", "user_id", "user_name", "dateTimeUTC", "tweet", "lng", and "lat"
#' @export
#'
#' @examples
#' pol <- st_sf(polyid=1, st_sfc(st_polygon(list(cbind(c(-120,-120,-90,-90, -120),c(20,50,50,20, 20)))), crs = 4326))
#' pull_tweets_usa(dates = as.IDate("2014-12-01"), words = "test", n_max = 1000, poly = pol)
pull_tweets_usa <- function(dates = NULL, poly = NULL, words = NULL, n_max = Inf) {
# DEBUG
# dates <- as.IDate(c("2016-01-01", "2016-01-03", "2015-10-05", "2017-08-03", "2017-03-04"))
# words = c("and", "I")
# poly = st_read("data/tl_2010_us_cbsa10/tl_2010_us_cbsa10.shp")[, "GEOID10"]
# n_max = 100
# END DEBUG
TOLD <- file.path("/data2/tweets_bak/usa_geo_backup/data/db_to_csv")
TNEW <- file.path("/data2/tweets_bak/usa_geo_stream/flat/uncompressed")
if (is.null(dates)) { # Set dates from before Twitter started to present day
dates_use <- seq(as.IDate("2006-01-01"), as.IDate(Sys.Date()), 1)
} else {
dates_use <- dates
}
# Load the list of all files, keep only the ones with the right dates
# files_old are stored by date
files_old <- data.table(fn = list.files(TOLD, pattern = "\\.csv", full.names=T))
files_old[, date := as.IDate(str_extract(basename(fn), "[0-9]{4}-[0-9]{2}-[0-9]{2}"))]
files_old <- files_old[date %in% dates_use]
tweets_old <- rbindlist(lapply(files_old$fn, pull_tweets_file,
col_types = cols_only(tweet_id = "c", user_id = "c", user_name = "c", tweet_datetime = "T", tweet = "c", lng="d", lat="d"),
col_names = c("tweet_id", "user_id", "user_name", "dateTimeUTC", "tweet", "lng", "lat"),
words = words, poly = poly, n_max = n_max))
# New files are stored by ym (in retrospect, a stupid decision that I should fix later)
files_new <- data.table(fn = list.files(TNEW, pattern = "\\.csv", full.names=T))
temp <- str_extract(basename(files_new$fn), "[0-9]{4}[0-9]{2}")
files_new[, ym := as.yearmon(as.numeric(substr(temp, 1, 4)) + (as.numeric(substr(temp, 5, 6)) - 1) / 12)]
yms <- as.yearmon(dates_use)
files_new <- files_new[ym %in% yms]
# I pass dates rather than dates_use so that pull_tweets_file can skip the date filtering step if NULL is passed
tweets_new <- rbindlist(lapply(files_new$fn, pull_tweets_file,
col_types = cols_only(id = "c", userid = "c", userName = "c", postedTime = "c", tweet = "c", lng="d", lat="d"),
col_names = c("tweet_id", "user_id", "user_name", "dateTimeUTC", "tweet", "lng", "lat"),
words = words, poly = poly, n_max = n_max, dates = dates))
rbindlist(list(tweets_old, tweets_new))
}
#' @return data.table of tweets
#' @export
#'
#' @examples
#' pull_tweets_file(in_file = "/data2/tweets_bak/usa_geo_stream/flat/uncompressed/tweets_201701.csv",
#' col_types = cols_only(id = "c", userid = "c", userName = "c", postedTime = "c", tweet = "c", lng="d", lat="d"),
#' col_names = c("tweet_id", "user_id", "user_name", "dateTimeUTC", "tweet", "lng", "lat"),
#' words = c("test"),
#' poly = pol,
#' n_max = 1000)
pull_tweets_file <- function(in_file, col_types, col_names = NULL, words = NULL, poly = NULL, n_max = Inf,
dates = NULL) {
# START DEBUG
# in_file <- files_old$fn[2]; col_types = cols_only(tweet_id = "c", user_id = "c", user_name = "c", tweet_datetime = "T", tweet = "c", lng="d", lat="d")
# in_file <- files_new$fn[1]; col_types = cols_only(id = "c", userid = "c", userName = "c", postedTime = "c", tweet = "c", lng="d", lat="d")
# col_names = c("tweet_id", "user_id", "user_name", "dateTimeUTC", "tweet", "lng", "lat")
# n_max = 100; words = c("and", "I"); poly = st_read("data/tl_2010_us_cbsa10/tl_2010_us_cbsa10.shp")[, "GEOID10"]
# END DEBUG
print(in_file)
dt <- suppressWarnings(read_csv(in_file, col_types = col_types, n_max = n_max))
if (nrow(dt) < 1) { return(NULL) }
setDT(dt)
setcolorder(dt, names(col_types$cols)) # Reorder, read_csv doesn't respect order of col_types
if (!is.null(col_names)) { setnames(dt, col_names) } # Rename if names passed
if (!all(c("dateTimeUTC", "lng", "lat", "tweet") %in% names(dt))) {
stop("One of the following required columns not found: dateTimeUTC, lng, lat, tweet")
}
if ("character" %in% class(dt$dateTimeUTC)) { # Fix datetime if it's a string
# Assumes format per https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object
print("Converting character datetime to POSIXct")
dt[, dateTimeUTC := as.POSIXct(dateTimeUTC, format="%a %b %d %H:%M:%S +0000 %Y", tz = "UTC")]
}
if (!is.null(dates)) {
# Because some of the files are compiled for a year-month, sometimes we want only a few days
# within that yearmonth. Unfortunately there's no way around loading the whole thing, but
# this lets us filter out irrelevant dates before the costly word and poly filtering
print("Filtering by date")
dt <- dt[as.IDate(dateTimeUTC) %in% dates]
if (nrow(dt) < 1) { return(NULL) }
}
# Filtering by polygon is time-consuming, so it's better to first filter on words
if (!is.null(words)) {
print("Filtering on word matches")
dt <- dt[grepl(paste(tolower(words), collapse = "|"), tolower(tweet))]
if (nrow(dt) < 1) { return(NULL) }
}
if (!is.null(poly)) {
print("Filtering by polygon(s)")
if (ncol(poly) > 2) {warning("FYI: poly has more than 2 columns, all columns will be attached to dataset. Could cause memory issues.")}
dt <- st_as_sf(dt, coords = c("lng", "lat"),  crs = 4326, agr = "constant", remove=F)
dt <- st_join(st_transform(dt, st_crs(poly)), poly, left = F)
setDT(dt)
if (nrow(dt) < 1) { return(NULL) }
# Can compute local date/time if we have geometry
dt[, c("localDate", "localTime") := get_local_time(dateTimeUTC, geometry)]
dt[, `:=`(geometry = NULL)]
}
dt
}
# Pull tweets for the USA for a limited time frame ----
cbsas <- st_read("data/tl_2010_us_cbsa10/tl_2010_us_cbsa10.shp")[, "GEOID10"]
# Extract pre-2017 and post-2017 separately, then combine. Not required but easier to do
metoo_tweets_before2017 <- pull_tweets_usa(dates = seq(as.IDate("2006-01-01"), as.IDate("2017-01-01"), 1),
words = c("#metoo", "#timesup"),
poly = cbsas)
metoo_tweets_before2017 <- readRDS("data/metoo_tweets_before2017.Rds")
metoo_tweets_after2017 <- readRDS("data/metoo_tweets_after2017.Rds")
metoo_tweets <- rbind(metoo_tweets_before2017, metoo_tweets_after2017)
metoo_tweets <- metoo_tweets %>% mutate(metoo = as.numeric(grepl("#metoo", tolower(tweet)))) %>%
mutate(metoo = factor(metoo, levels = c(0, 1), labels = c("#timesup", "#metoo")))
metoo_bydate <- metoo_tweets %>% group_by(localDate, metoo) %>% summarise(N = n())
metoo_bydate <- merge(metoo_bydate,
expand.grid(localDate = seq(min(metoo_tweets$localDate, na.rm = T),
max(metoo_tweets$localDate, na.rm = T), 1),
metoo = c("#metoo", "#timesup")),
by = c("localDate", "metoo"), all.y = T)
metoo_tweets_before2017 <- readRDS("data/metoo_tweets_before2017.Rds")
metoo_tweets_after2017 <- readRDS("data/metoo_tweets_after2017.Rds")
metoo_tweets <- rbind(metoo_tweets_before2017, metoo_tweets_after2017)
metoo_tweets <- metoo_tweets %>% mutate(metoo = as.numeric(grepl("#metoo", tolower(tweet)))) %>%
mutate(metoo = factor(metoo, levels = c(0, 1), labels = c("#timesup", "#metoo")))
metoo_bydate <- metoo_tweets %>% group_by(localDate, metoo) %>% summarise(N = n())
metoo_bydate <- merge(metoo_bydate,
expand.grid(localDate = seq(min(metoo_tweets$localDate, na.rm = T),
max(metoo_tweets$localDate, na.rm = T), 1),
metoo = c("#metoo", "#timesup")),
by = c("localDate", "metoo"), all.y = T)
ggplot(metoo_bydate) + geom_line(mapping = aes(x = localDate, y = N, colour = metoo, group = metoo)) +
scale_y_continuous(name = "Daily count of geotagged tweets", label = comma) +
labs(x = NULL) +
scale_colour_discrete(name = NULL) +
theme_tufte()
source("config.R",echo=TRUE)
ggplot(metoo_bydate %>% filter(localDate >= as.IDate("2017-10-01"))) + geom_line(mapping = aes(x = localDate, y = N, colour = metoo, group = metoo)) +
scale_y_continuous(name = "Daily count of geotagged tweets", label = comma) +
scale_x_date(name = NULL, date_breaks = "1 month", date_labels =  "%b %Y") +
scale_colour_discrete(name = NULL) +
theme_tufte()
ggplot(metoo_bydate) + geom_line(mapping = aes(x = localDate, y = N, colour = metoo, group = metoo)) +
scale_y_continuous(name = "Daily count of geotagged tweets", label = comma) +
labs(x = NULL) +
scale_colour_discrete(name = NULL) +
theme_tufte()
